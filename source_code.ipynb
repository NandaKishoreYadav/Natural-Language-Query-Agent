{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "915a0b61",
   "metadata": {},
   "source": [
    "<h1> Importing Necessary Libraries</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bd378b",
   "metadata": {},
   "source": [
    "Embeddings and Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e285dbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6673df42",
   "metadata": {},
   "source": [
    "Chains and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1c1c0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9449dfdf",
   "metadata": {},
   "source": [
    "PDF Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0d1aa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ea9b0c",
   "metadata": {},
   "source": [
    "Typing Extensions and Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2a0314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Concatenate\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a90d83",
   "metadata": {},
   "source": [
    "Conversational Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75ca0020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c98183",
   "metadata": {},
   "source": [
    "<h2> Loading a PDF File\n",
    "To load a PDF file and initialize a PdfReader object for further processing: </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "922e22a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PdfReader('lecture_notes.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d259c44",
   "metadata": {},
   "source": [
    "<h2>Extracting Text from PDF Pages</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "663bf10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To extract text from each page of a loaded PDF document using PdfReader:\n",
    "\n",
    "raw_text = ''\n",
    "for i, page in enumerate(loader.pages):\n",
    "    content = page.extract_text()\n",
    "    if content:\n",
    "        raw_text += content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "853c2e80",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are large language models (LLMs)?\\nLarge language models (LLMs) are a category of foundation models trained on immense amounts of data making them\\ncapable of understanding and generating natural language and other types of content to perform a wide range of tasks.\\nLLMs have become a household name thanks to the role they have played in bringing generative AI to the forefront of\\nthe public interest, as well as the point on which organizations are focusing to adopt artificial intelligence across\\nnumerous business functions and use cases.\\nLLMs are a class of foundation models, which are trained on enormous amounts of data to provide the foundational\\ncapabilities needed to drive multiple use cases and applications, as well as resolve a multitude of tasks. This is in stark\\ncontrast to the idea of building and training domain specific models for each of these use cases individually, which is\\nprohibitive under many criteria (most importantly cost and infrastructure), stifles synergies and can even lead to inferior\\nperformance.\\nLLMs represent a significant breakthrough in NLP and artificial intelligence, and are easily accessible to the public\\nthrough interfaces like Open AI’s Chat GPT-3 and GPT-4, which have garnered the support of Microsoft. Other\\nexamples include Meta’s Llama models and Google’s bidirectional encoder representations from transformers\\n(BERT/RoBERTa) and PaLM models. IBM has also recently launched its Granite model series on watsonx.ai, which has\\nbecome the generative AI backbone for other IBM products like watsonx Assistant and watsonx Orchestrate. \\nIn a nutshell, LLMs are designed to understand and generate text like a human, in addition to other forms of content,\\nbased on the vast amount of data used to train them. They have the ability to infer from context, generate coherent and\\ncontextually relevant responses, translate to languages other than English, summarize text, answer questions (general\\nconversation and FAQs) and even assist in creative writing or code generation tasks. \\nThey are able to do this thanks to billions of parameters that enable them to capture intricate patterns in language and\\nperform a wide array of language-related tasks. LLMs are revolutionizing applications in various fields, from chatbots\\nand virtual assistants to content generation, research assistance and language translation.\\nAs they continue to evolve and improve, LLMs are poised to reshape the way we interact with technology and access\\ninformation, making them a pivotal part of the modern digital landscape.\\nTransformers and Attention Mechanism: The Backbone of LLMs\\nIntroduction\\nIn our journey through the universe of Large Language Models, we’ve traversed the basic neural network terrains and\\ndelved deep into deep learning. Now, it’s time to explore the architecture and mechanisms that form the backbone of\\nmost modern LLMs: Transformers and Attention Mechanisms.\\nSetting the Stage: From RNNs and LSTMs to Transformers\\nBefore the inception of the Transformer architecture, Recurrent Neural Networks (RNNs) and their evolved counterparts,\\nLong Short-Term Memory units (LSTMs), were the go-to structures for sequence-to-sequence tasks in NLP. They\\nprocessed sequences step-by-step, holding potential memory from previous steps. However, they had limitations,\\nespecially when dealing with long sequences, often leading to the vanishing gradient problem and difficulties in handling\\nlong-range dependencies.\\nEnter Transformers, which brought parallel processing to sequences and effectively addressed the issues of long-range\\ndependencies through self-attention mechanisms.\\nThe Transformer Architecture\\nTransformers, introduced in the paper “Attention is All You Need” by Vaswani et al., have become the foundation for\\nmost state-of-the-art NLP models.\\nKey Components:\\n1. Self-Attention Mechanism: Instead of processing sequences step-by-step, it allows the model to focus on different\\nparts of the input simultaneously.\\n2. Positional Encoding: Since Transformers don’t process data sequentially, they need a way to consider the position of\\nwords in a sequence. Positional encoding is added to give the model information about the position of words.\\n3. Feed-Forward Neural Networks: Each attention output is passed through a feed-forward neural network (the same\\none for each position).\\n4. Stacked Layers: Both the encoder and decoder of the transformer contain multiple identical layers stacked on top of\\neach other.\\nAttention Mechanism: What Does it Pay Attention to?\\nThe essence of the Attention Mechanism is its ability to focus on specific parts of the input data, akin to how humans\\npay attention to specific parts of a sentence while comprehending or responding.\\nThe attention mechanism computes a weighted sum of input values (or values from the previous layer), where theweights are decided based on the query, key, and value representations of the data.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369e8f5f",
   "metadata": {},
   "source": [
    "<h2> Splitting Text into Chunks </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ccd7ff",
   "metadata": {},
   "source": [
    "The very first step of loading our raw data to convert them into our knowledge base(vector DB). The Data Loading Stage.\n",
    "\n",
    "Raw Data → Converted to Chunks → Indexed as Vectors in VectorDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e1629a",
   "metadata": {},
   "source": [
    "Character Splitting: Naive Way\n",
    "Fixed length chunk with some overlap between successive chunks\n",
    "\n",
    "The easiest form of splitting with no regards to sentence structure or semantics.\n",
    "\n",
    "Chunk Size: is the number of characters ’N’ per split\n",
    "\n",
    "Chunk Overlap: Number of characters with overlap between successive chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8c12997",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 800,\n",
    "    chunk_overlap  = 200,\n",
    "    length_function = len,\n",
    ")\n",
    "texts = text_splitter.split_text(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d981d1cf",
   "metadata": {},
   "source": [
    "<h2> Initializing HuggingFace Embeddings </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650152b6",
   "metadata": {},
   "source": [
    "An embedding is a numerical representation of a piece of information, for example, text, documents, images, audio, etc. The representation captures the semantic meaning of what is being embedded, making it robust for many industry applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56a6f3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b753d7",
   "metadata": {},
   "source": [
    "<h2> Creating Document Objects </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e84129da",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [Document(page_content=text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33471f62",
   "metadata": {},
   "source": [
    "<h2> Initializing FAISS Vector Database </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9730ba",
   "metadata": {},
   "source": [
    "A vector database indexes and stores vector embeddings for fast retrieval and similarity search, with capabilities like CRUD operations, metadata filtering, horizontal scaling, and serverless."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e8b8c9",
   "metadata": {},
   "source": [
    "First, we use the embedding model to create vector embeddings for the content we want to index.\n",
    "The vector embedding is inserted into the vector database, with some reference to the original content the embedding was created from.\n",
    "When the application issues a query, we use the same embedding model to create embeddings for the query and use those embeddings to query the database for similar vector embeddings. As mentioned before, those similar embeddings are associated with the original content that was used to create them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ce07e9",
   "metadata": {},
   "source": [
    "Facebook AI Similarity Search (Faiss) is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "985fc280",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = FAISS.from_documents(documents=documents, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d63368",
   "metadata": {},
   "source": [
    "<h2> Initializing GooglePalm Language Model </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebc1901",
   "metadata": {},
   "source": [
    "PaLM 2 is our next generation language model with improved multilingual, reasoning and coding capabilities that builds on Google’s legacy of breakthrough research in machine learning and responsible AI.\n",
    "\n",
    "It excels at advanced reasoning tasks, including code and math, classification and question answering, translation and multilingual proficiency, and natural language generation better than our previous state-of-the-art LLMs, including PaLM. It can accomplish these tasks because of the way it was built – bringing together compute-optimal scaling, an improved dataset mixture, and model architecture improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb8289be",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = GooglePalm(google_api_key=\"YOUR_GOOGLEPALM_API\", temperature=0.1) # You can get it from https://aistudio.google.com/app/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0935356",
   "metadata": {},
   "source": [
    "<h2> Setting Up Retrieval-Based Question Answering System </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61f5cc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(score_threshold=0.7)\n",
    "\n",
    "prompt_template = \"\"\"Given the following context and a question, generate an answer based on this context only. \n",
    "If applicable, include the source of the information. If the answer is not found in the context, state \"I don't know.\"\n",
    "Do not provide extra information or make up an answer.\n",
    "\n",
    "CONTEXT: {context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    input_key=\"query\",\n",
    "    return_source_documents=True,\n",
    "    output_key='result',\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ebf14f7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'what is llm',\n",
       " 'result': 'A large language model (LLM) is a category of foundation models trained on immense amounts of data making them\\ncapable of understanding and generating natural language and other types of content to perform a wide range of tasks.',\n",
       " 'source_documents': [Document(page_content='What are large language models (LLMs)?\\nLarge language models (LLMs) are a category of foundation models trained on immense amounts of data making them\\ncapable of understanding and generating natural language and other types of content to perform a wide range of tasks.\\nLLMs have become a household name thanks to the role they have played in bringing generative AI to the forefront of\\nthe public interest, as well as the point on which organizations are focusing to adopt artificial intelligence across\\nnumerous business functions and use cases.\\nLLMs are a class of foundation models, which are trained on enormous amounts of data to provide the foundational\\ncapabilities needed to drive multiple use cases and applications, as well as resolve a multitude of tasks. This is in stark'),\n",
       "  Document(page_content='capabilities needed to drive multiple use cases and applications, as well as resolve a multitude of tasks. This is in stark\\ncontrast to the idea of building and training domain specific models for each of these use cases individually, which is\\nprohibitive under many criteria (most importantly cost and infrastructure), stifles synergies and can even lead to inferior\\nperformance.\\nLLMs represent a significant breakthrough in NLP and artificial intelligence, and are easily accessible to the public\\nthrough interfaces like Open AI’s Chat GPT-3 and GPT-4, which have garnered the support of Microsoft. Other\\nexamples include Meta’s Llama models and Google’s bidirectional encoder representations from transformers'),\n",
       "  Document(page_content='conversation and FAQs) and even assist in creative writing or code generation tasks. \\nThey are able to do this thanks to billions of parameters that enable them to capture intricate patterns in language and\\nperform a wide array of language-related tasks. LLMs are revolutionizing applications in various fields, from chatbots\\nand virtual assistants to content generation, research assistance and language translation.\\nAs they continue to evolve and improve, LLMs are poised to reshape the way we interact with technology and access\\ninformation, making them a pivotal part of the modern digital landscape.\\nTransformers and Attention Mechanism: The Backbone of LLMs\\nIntroduction\\nIn our journey through the universe of Large Language Models, we’ve traversed the basic neural network terrains and'),\n",
       "  Document(page_content='examples include Meta’s Llama models and Google’s bidirectional encoder representations from transformers\\n(BERT/RoBERTa) and PaLM models. IBM has also recently launched its Granite model series on watsonx.ai, which has\\nbecome the generative AI backbone for other IBM products like watsonx Assistant and watsonx Orchestrate. \\nIn a nutshell, LLMs are designed to understand and generate text like a human, in addition to other forms of content,\\nbased on the vast amount of data used to train them. They have the ability to infer from context, generate coherent and\\ncontextually relevant responses, translate to languages other than English, summarize text, answer questions (general\\nconversation and FAQs) and even assist in creative writing or code generation tasks.')]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain('what is llm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f554b566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A large language model (LLM) is a category of foundation models trained on immense amounts of data making them\\ncapable of understanding and generating natural language and other types of content to perform a wide range of tasks.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain('what is llm')['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d47efc31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Large language models'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain(\"llm full form\")['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "99efac4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain(\"IOT full form\")['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e56fddd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain(\"what is the previous question asked\")['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beca2cfd",
   "metadata": {},
   "source": [
    "In the current system setup, there is no provision for retaining context from past interactions, limiting the ability to maintain continuity in conversations or provide personalized responses based on historical interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83b7326",
   "metadata": {},
   "source": [
    "<h2> Adding Conversation Memory </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff37e01",
   "metadata": {},
   "source": [
    "To enhance the interaction capabilities of the system by incorporating memory for previous conversations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d66abe9",
   "metadata": {},
   "source": [
    "<h2> Integrating Conversation Memory into Retrieval-Based QA System </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5529daad",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Given the following context and a question, generate an answer based on this context only and also check the history if asking any previous. \n",
    "If applicable, include the source of the information. If the answer is not found in the context, state \"I don't know.\"\n",
    "Do not provide extra information or make up an answer.\n",
    "\n",
    "CONTEXT: {context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=3,output_key='result')\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    input_key=\"query\",\n",
    "    return_source_documents=True,\n",
    "    output_key='result',\n",
    "    chain_type_kwargs={\"prompt\": PROMPT},\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60792324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is LLM',\n",
       " 'history': '',\n",
       " 'result': 'LLMs are a category of foundation models trained on immense amounts of data making them capable of understanding and generating natural language and other types of content to perform a wide range of tasks.',\n",
       " 'source_documents': [Document(page_content='What are large language models (LLMs)?\\nLarge language models (LLMs) are a category of foundation models trained on immense amounts of data making them\\ncapable of understanding and generating natural language and other types of content to perform a wide range of tasks.\\nLLMs have become a household name thanks to the role they have played in bringing generative AI to the forefront of\\nthe public interest, as well as the point on which organizations are focusing to adopt artificial intelligence across\\nnumerous business functions and use cases.\\nLLMs are a class of foundation models, which are trained on enormous amounts of data to provide the foundational\\ncapabilities needed to drive multiple use cases and applications, as well as resolve a multitude of tasks. This is in stark'),\n",
       "  Document(page_content='capabilities needed to drive multiple use cases and applications, as well as resolve a multitude of tasks. This is in stark\\ncontrast to the idea of building and training domain specific models for each of these use cases individually, which is\\nprohibitive under many criteria (most importantly cost and infrastructure), stifles synergies and can even lead to inferior\\nperformance.\\nLLMs represent a significant breakthrough in NLP and artificial intelligence, and are easily accessible to the public\\nthrough interfaces like Open AI’s Chat GPT-3 and GPT-4, which have garnered the support of Microsoft. Other\\nexamples include Meta’s Llama models and Google’s bidirectional encoder representations from transformers'),\n",
       "  Document(page_content='conversation and FAQs) and even assist in creative writing or code generation tasks. \\nThey are able to do this thanks to billions of parameters that enable them to capture intricate patterns in language and\\nperform a wide array of language-related tasks. LLMs are revolutionizing applications in various fields, from chatbots\\nand virtual assistants to content generation, research assistance and language translation.\\nAs they continue to evolve and improve, LLMs are poised to reshape the way we interact with technology and access\\ninformation, making them a pivotal part of the modern digital landscape.\\nTransformers and Attention Mechanism: The Backbone of LLMs\\nIntroduction\\nIn our journey through the universe of Large Language Models, we’ve traversed the basic neural network terrains and'),\n",
       "  Document(page_content='examples include Meta’s Llama models and Google’s bidirectional encoder representations from transformers\\n(BERT/RoBERTa) and PaLM models. IBM has also recently launched its Granite model series on watsonx.ai, which has\\nbecome the generative AI backbone for other IBM products like watsonx Assistant and watsonx Orchestrate. \\nIn a nutshell, LLMs are designed to understand and generate text like a human, in addition to other forms of content,\\nbased on the vast amount of data used to train them. They have the ability to infer from context, generate coherent and\\ncontextually relevant responses, translate to languages other than English, summarize text, answer questions (general\\nconversation and FAQs) and even assist in creative writing or code generation tasks.')]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain('What is LLM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "551992f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain(\"IOT full form\")['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c23c258f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The first question asked is \"What are Large Language Models?\"'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain(\"what is the first question asked\")['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a3782fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Transformer, introduced in the paper “Attention is All You Need” by Vaswani et al., have become the foundation for\\nmost state-of-the-art NLP models.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain(\"What are some milestone model architectures and papers in the last few years?\")['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa2ce68c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Self-Attention Mechanism, Positional Encoding, Feed-Forward Neural Networks'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain(\"What are the layers in a transformer block?\")['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4dfcd562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LLMs are trained on massive datasets of text, code, and other forms of content. The size of these datasets can range\\nfrom hundreds of millions to billions of words. The most common sources of data for LLMs are Wikipedia, news articles,\\nand books. However, LLMs can also be trained on data from social media, email, and other sources.\\n\\nThe process of cleaning data for LLMs is complex and challenging. The data must be free of errors and inconsistencies, and\\nit must be structured in a way that the LLM can understand. Cleaning data for LLMs typically involves removing\\nduplicates, correcting errors, and normalizing text.\\n\\nThere are a number of tools and techniques that can be used to clean data for LLMs. Some of the most common\\ntechniques include:\\n\\n* **Tokenization:** This process breaks text into individual words or tokens.\\n* **Stemming:** This process removes the endings of words, such as \"-ed\" and \"-ing\".\\n* **Lemmatization:** This process reduces words to their base form, such as \"run\" to \"run\".\\n* **Part-of-speech tagging:** This process assigns each word in a sentence a part-of-speech tag, such as \"noun\" or\\n\"verb\".\\n* **Named entity recognition:** This process identifies and labels named entities in a text, such as people, places, and\\norganizations.\\n\\nThe process of cleaning data for LLMs is essential to ensuring that the models are accurate and reliable. By removing\\nerrors and inconsistencies, and by structuring the data in a way that the LLM can understand, the cleaning process\\nhelps to improve the performance of the models.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain(\"Tell me about datasets used to train LLMs and how they are cleaned\")['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e94624ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am a large language model (LLM) that was trained on a massive dataset of text and code. I am able to understand and generate natural language, translate languages, write different kinds of creative content, and answer your questions. I am still under development, but I am learning new things every day.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain(\"who are you\")['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba274dad",
   "metadata": {},
   "source": [
    "<h2> Intoducing Agents</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d34d2734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SERPAPI_API_KEY'] = \"YOUR_SERP_API\"    # You can get your free API from https://serpapi.com/manage-api-key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c93547",
   "metadata": {},
   "source": [
    "<h2> Initializing Agent with Tools and LLM </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e6e145bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType, initialize_agent , load_tools\n",
    "\n",
    "tools = load_tools([\"serpapi\",\"llm-math\",\"wikipedia\"], llm = llm)\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "375dd926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to know the fullform of IOT\n",
      "Action: wikipedia\n",
      "Action Input: IOT\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NandaKishore\\AppData\\Roaming\\Python\\Python311\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\NandaKishore\\AppData\\Roaming\\Python\\Python311\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mPage: Internet of things\n",
      "Summary: The Internet of things (IoT) describes devices with sensors, processing ability, software and other technologies that connect and exchange data with other devices and systems over the Internet or other communications networks. The Internet of things encompasses electronics, communication, and computer science engineering. \"Internet of things\" has been considered a misnomer because devices do not need to be connected to the public internet; they only need to be connected to a network and be individually addressable.\n",
      "The field has evolved due to the convergence of multiple technologies, including ubiquitous computing, commodity sensors, and increasingly powerful embedded systems, as well as machine learning. Older fields of embedded systems, wireless sensor networks, control systems, automation (including home and building automation), independently and collectively enable the Internet of things.  In the consumer market, IoT technology is most synonymous with \"smart home\" products, including devices and appliances (lighting fixtures, thermostats, home security systems, cameras, and other home appliances) that support one or more common ecosystems and can be controlled via devices associated with that ecosystem, such as smartphones and smart speakers. IoT is also used in healthcare systems.\n",
      "There are a number of concerns about the risks in the growth of IoT technologies and products, especially in the areas of privacy and security, and consequently there have been industry and government moves to address these concerns, including the development of international and local standards, guidelines, and regulatory frameworks. Because of their interconnected nature, IoT devices are vulnerable to security breaches and privacy concerns. At the same time, the way these devices communicate wirelessly creates regulatory ambiguities, complicating jurisdictional boundaries of the data transfer.\n",
      "\n",
      "Page: Windows IoT\n",
      "Summary: Windows IoT, short for Windows Internet of Things and formerly known as Windows Embedded, is a family of operating systems from Microsoft designed for use in embedded systems. Microsoft has three different subfamilies of operating systems for embedded devices targeting a wide market, ranging from small-footprint, real-time devices to point of sale (POS) devices like kiosks. Windows Embedded operating systems are available to original equipment manufacturers (OEMs), who make it available to end users preloaded with their hardware, in addition to volume license customers in some cases.\n",
      "In April 2018, Microsoft released Azure Sphere, another operating system designed for IoT applications running on the Linux kernel.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe fullform of IOT is Internet of Things\n",
      "Final Answer: Internet of Things\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Internet of Things'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"What is fullform of IOT?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "39e56095",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to find out India's population in 2024\n",
      "Action: wikipedia\n",
      "Action Input: India's population\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mPage: Demographics of India\n",
      "Summary: India is the most populous country in the world with one-sixth of the world's population. \n",
      "According to estimates from the United Nations (UN), India has overtaken China as the country with the largest population in the world, with a population of 1,425,775,850 at the end of April 2023.\n",
      "Between 1975 and 2010, the population doubled to 1.2 billion, reaching the billion mark in 2000. According to the UN's World Population dashboard, India's population now stands at slightly over 1.428 billion, edging past China's population of 1.425 billion people, as reported by the news agency Bloomberg. Its population is set to reach 1.7 billion by 2050. In 2017 its population growth rate was 0.98%, ranking 112th in the world; in contrast, from 1972 to 1983, India's population grew by an annual rate of 2.3%.\n",
      "In 2022, the median age of an Indian was 28.7 years, compared to 38.4 for China and 48.6 for Japan; and, by 2030; India's dependency ratio will be just over 0.4. However, the number of children in India peaked more than a decade ago and is now falling. The number of children under the age of five peaked in 2007, and since then the number has been falling. The number of Indians under 15 years old peaked slightly later (in 2011) and is now also declining.\n",
      "India has many ethnic groups, and every major religion is represented, as are four major families of languages (Indo-European, Dravidian, Austroasiatic and Sino-Tibetan languages) as well as two language isolates: the Nihali language, spoken in parts of Maharashtra, and the Burushaski language, spoken in parts of Jammu and Kashmir. 1,000,000 people in India are Anglo-Indians and 700,000 United States citizens are living in India. They represent over 0.1% of the total population of India. Overall, only the continent of Africa exceeds the linguistic, genetic and cultural diversity of the nation of India.\n",
      "The sex ratio was 944 females for 1000 males in 2016, and 940 per 1000 in 2011. This ratio has been showing an upwards trend for the last two decades after a continuous decline in the 20th century.\n",
      "\n",
      "\n",
      "\n",
      "Page: List of cities in India by population\n",
      "Summary: The following tables are the list of cities in India by population. Often cities are bifurcated into multiple regions (municipalities) which results in creation of cities within cities which may figure in the list. The entire work of this article is based on the census of India using data from the 2001 census of India and the 2011 census of India, conducted by the Office of the Registrar General and Census Commissioner, under Ministry of Home Affairs, Government of India.\n",
      "\n",
      "Page: List of states and union territories of India by population\n",
      "Summary: India is a union consisting of 28 states and 8 union territories. As of 2022, with an estimated population of 1.4 billion, India is the world's most populous country. India occupies 2.4% of the world's area and is home to 17.5% of the world's population. The Indo-Gangetic Plain has one of the world's biggest stretches of fertile not-deep alluvium and are among the most densely populated areas of the world. The eastern and western coastal regions of Deccan Plateau are also densely populated regions of India. The Thar Desert in western Rajasthan is one of the most densely populated deserts in the world. The northern and north-eastern states along the Himalayas contain cold arid deserts with fertile valleys. These states have relatively low population density due to indomitable physical barriers.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mIndia's population in 2024 is 1.425,775,850\n",
      "Final Answer: 1.425,775,850\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.425,775,850'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"What is India's Population in 2024?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
